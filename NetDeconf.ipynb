{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "import torch\n",
        "\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn.modules.module import Module\n",
        "\n",
        "\n",
        "class GraphConvolution(Module):\n",
        "    \"\"\"\n",
        "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features, bias=True):\n",
        "        super(GraphConvolution, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
        "        self.weight.data.uniform_(-stdv, stdv)\n",
        "        if self.bias is not None:\n",
        "            self.bias.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, input, adj):\n",
        "        support = torch.mm(input, self.weight)\n",
        "        output = torch.spmm(adj, support)\n",
        "        if self.bias is not None:\n",
        "            return output + self.bias\n",
        "        else:\n",
        "            return output\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + ' (' \\\n",
        "               + str(self.in_features) + ' -> ' \\\n",
        "               + str(self.out_features) + ')'"
      ],
      "metadata": {
        "id": "hcxNBXAz0Ayw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZvSr9YvAzpIq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class GCN_DECONF(nn.Module):\n",
        "    def __init__(self, nfeat, nhid, dropout, n_in=1, n_out=1, cuda=False):\n",
        "        super(GCN_DECONF, self).__init__()\n",
        "\n",
        "\n",
        "        # self.gc2 = GraphConvolution(nhid, nclass)\n",
        "\n",
        "        if cuda:\n",
        "            self.gc = [GraphConvolution(nfeat, nhid).cuda()]\n",
        "            for i in range(n_in - 1):\n",
        "                self.gc.append(GraphConvolution(nhid, nhid).cuda())\n",
        "        else:\n",
        "            self.gc = [GraphConvolution(nfeat, nhid)]\n",
        "            for i in range(n_in - 1):\n",
        "                self.gc.append(GraphConvolution(nhid, nhid))\n",
        "\n",
        "\n",
        "        print(self.gc)\n",
        "\n",
        "        self.n_in = n_in\n",
        "        self.n_out = n_out\n",
        "\n",
        "        if cuda:\n",
        "\n",
        "            self.out_t00 = [nn.Linear(nhid,nhid).cuda() for i in range(n_out)]\n",
        "            self.out_t10 = [nn.Linear(nhid,nhid).cuda() for i in range(n_out)]\n",
        "            self.out_t01 = nn.Linear(nhid,1).cuda()\n",
        "            self.out_t11 = nn.Linear(nhid,1).cuda()\n",
        "\n",
        "        else:\n",
        "            self.out_t00 = [nn.Linear(nhid,nhid) for i in range(n_out)]\n",
        "            self.out_t10 = [nn.Linear(nhid,nhid) for i in range(n_out)]\n",
        "            self.out_t01 = nn.Linear(nhid,1)\n",
        "            self.out_t11 = nn.Linear(nhid,1)\n",
        "\n",
        "        self.dropout = dropout\n",
        "\n",
        "        # a linear layer for propensity prediction\n",
        "        self.pp = nn.Linear(nhid, 1)\n",
        "\n",
        "        if cuda:\n",
        "            self.pp = self.pp.cuda()\n",
        "        self.pp_act = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x, adj, t, cuda=False):\n",
        "\n",
        "        rep = F.relu(self.gc[0](x, adj))\n",
        "        rep = F.dropout(rep, self.dropout, training=self.training)\n",
        "        for i in range(1, self.n_in):\n",
        "            rep = F.relu(self.gc[i](rep, adj))\n",
        "            rep = F.dropout(rep, self.dropout, training=self.training)\n",
        "\n",
        "        for i in range(self.n_out):\n",
        "\n",
        "            y00 = F.relu(self.out_t00[i](rep))\n",
        "            y00 = F.dropout(y00, self.dropout, training=self.training)\n",
        "            y10 = F.relu(self.out_t10[i](rep))\n",
        "            y10 = F.dropout(y10, self.dropout, training=self.training)\n",
        "\n",
        "        y0 = self.out_t01(y00).view(-1)\n",
        "        y1 = self.out_t11(y10).view(-1)\n",
        "\n",
        "        # print(t.shape,y1.shape,y0.shape)\n",
        "        y = torch.where(t > 0,y1,y0)\n",
        "\n",
        "        p1 = self.pp_act(self.pp(rep)).view(-1)\n",
        "\n",
        "        return y, rep, p1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.io as sio\n",
        "import scipy.sparse as sp\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import cycle\n",
        "\n",
        "from sklearn import svm, datasets\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "\n",
        "def sparse_mx_to_torch_sparse_tensor(sparse_mx,cuda=False):\n",
        "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
        "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
        "    indices = torch.from_numpy(\n",
        "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
        "    values = torch.from_numpy(sparse_mx.data)\n",
        "    shape = torch.Size(sparse_mx.shape)\n",
        "\n",
        "    sparse_tensor = torch.sparse.FloatTensor(indices, values, shape)\n",
        "    if cuda:\n",
        "        sparse_tensor = sparse_tensor.cuda()\n",
        "    return sparse_tensor\n",
        "\n",
        "def normalize(mx):\n",
        "\t\"\"\"Row-normalize sparse matrix\"\"\"\n",
        "\trowsum = np.array(mx.sum(1))\n",
        "\tr_inv = np.power(rowsum, -1).flatten()\n",
        "\tr_inv[np.isinf(r_inv)] = 0.\n",
        "\tr_mat_inv = sp.diags(r_inv)\n",
        "\tmx = r_mat_inv.dot(mx)\n",
        "\treturn mx\n",
        "\n",
        "def load_data(path, name='BlogCatalog',exp_id='0',original_X = False, extra_str=\"\"):\n",
        "\tdata = sio.loadmat(path+name+extra_str+'/'+name+exp_id+'.mat')\n",
        "\tA = data['Network'] #csr matrix\n",
        "\n",
        "\t# try:\n",
        "\t# \tA = np.array(A.todense())\n",
        "\t# except:\n",
        "\t# \tpass\n",
        "\n",
        "\tif not original_X:\n",
        "\t\tX = data['X_100']\n",
        "\telse:\n",
        "\t\tX = data['Attributes']\n",
        "\n",
        "\tY1 = data['Y1']\n",
        "\tY0 = data['Y0']\n",
        "\tT = data['T']\n",
        "\n",
        "\treturn X, A, T, Y1, Y0\n",
        "\n",
        "\n",
        "def wasserstein(x,y,p=0.5,lam=10,its=10,sq=False,backpropT=False,cuda=False):\n",
        "    \"\"\"return W dist between x and y\"\"\"\n",
        "    '''distance matrix M'''\n",
        "    nx = x.shape[0]\n",
        "    ny = y.shape[0]\n",
        "\n",
        "    x = x.squeeze()\n",
        "    y = y.squeeze()\n",
        "\n",
        "#    pdist = torch.nn.PairwiseDistance(p=2)\n",
        "\n",
        "    M = pdist(x,y) #distance_matrix(x,y,p=2)\n",
        "\n",
        "    '''estimate lambda and delta'''\n",
        "    M_mean = torch.mean(M)\n",
        "    M_drop = F.dropout(M,10.0/(nx*ny))\n",
        "    delta = torch.max(M_drop).detach()\n",
        "    eff_lam = (lam/M_mean).detach()\n",
        "\n",
        "    '''compute new distance matrix'''\n",
        "    Mt = M\n",
        "    # Explicitly create tensors on the correct device\n",
        "    row = delta*torch.ones(M[0:1,:].shape, device=x.device)\n",
        "    col = torch.cat([delta*torch.ones(M[:,0:1].shape, device=x.device),torch.zeros((1,1), device=x.device)],0)\n",
        "    if cuda:\n",
        "        row = row.cuda()\n",
        "        col = col.cuda()\n",
        "    Mt = torch.cat([M,row],0)\n",
        "    Mt = torch.cat([Mt,col],1)\n",
        "\n",
        "    '''compute marginal'''\n",
        "    a = torch.cat([p*torch.ones((nx,1), device=x.device)/nx,(1-p)*torch.ones((1,1), device=x.device)],0)\n",
        "    b = torch.cat([(1-p)*torch.ones((ny,1), device=x.device)/ny, p*torch.ones((1,1), device=x.device)],0)\n",
        "\n",
        "    '''compute kernel'''\n",
        "    Mlam = eff_lam * Mt\n",
        "    temp_term = torch.ones(1, device=x.device)*1e-6\n",
        "    if cuda:\n",
        "        temp_term = temp_term.cuda()\n",
        "        a = a.cuda()\n",
        "        b = b.cuda()\n",
        "    K = torch.exp(-Mlam) + temp_term\n",
        "    U = K * Mt\n",
        "    ainvK = K/a\n",
        "\n",
        "    u = a\n",
        "\n",
        "    for i in range(its):\n",
        "        u = 1.0/(ainvK.matmul(b/torch.t(torch.t(u).matmul(K))))\n",
        "        if cuda:\n",
        "            u = u.cuda()\n",
        "    v = b/(torch.t(torch.t(u).matmul(K)))\n",
        "    if cuda:\n",
        "        v = v.cuda()\n",
        "\n",
        "    upper_t = u*(torch.t(v)*K).detach()\n",
        "\n",
        "    E = upper_t*Mt\n",
        "    D = 2*torch.sum(E)\n",
        "\n",
        "    if cuda:\n",
        "        D = D.cuda()\n",
        "\n",
        "    return D, Mlam\n",
        "\n",
        "def pdist(sample_1, sample_2, norm=2, eps=1e-5):\n",
        "    \"\"\"Compute the matrix of all squared pairwise distances.\n",
        "    Arguments\n",
        "    ---------\n",
        "    sample_1 : torch.Tensor or Variable\n",
        "        The first sample, should be of shape ``(n_1, d)``.\n",
        "    sample_2 : torch.Tensor or Variable\n",
        "        The second sample, should be of shape ``(n_2, d)``.\n",
        "    norm : float\n",
        "        The l_p norm to be used.\n",
        "    Returns\n",
        "    -------\n",
        "    torch.Tensor or Variable\n",
        "        Matrix of shape (n_1, n_2). The [i, j]-th entry is equal to\n",
        "        ``|| sample_1[i, :] - sample_2[j, :] ||_p``.\"\"\"\n",
        "    n_1, n_2 = sample_1.size(0), sample_2.size(0)\n",
        "    norm = float(norm)\n",
        "    if norm == 2.:\n",
        "        norms_1 = torch.sum(sample_1**2, dim=1, keepdim=True)\n",
        "        norms_2 = torch.sum(sample_2**2, dim=1, keepdim=True)\n",
        "        norms = (norms_1.expand(n_1, n_2) +\n",
        "                 norms_2.transpose(0, 1).expand(n_1, n_2))\n",
        "        distances_squared = norms - 2 * sample_1.mm(sample_2.t())\n",
        "        return torch.sqrt(eps + torch.abs(distances_squared))\n",
        "    else:\n",
        "        dim = sample_1.size(1)\n",
        "        expanded_1 = sample_1.unsqueeze(1).expand(n_1, n_2, dim)\n",
        "        expanded_2 = sample_2.unsqueeze(0).expand(n_1, n_2, dim)\n",
        "        differences = torch.abs(expanded_1 - expanded_2) ** norm\n",
        "        inner = torch.sum(differences, dim=2, keepdim=False)\n",
        "        return (eps + inner) ** (1. / norm)\n",
        "\n",
        "# def sklearn_auc_score(t,ps):\n",
        "#     \"\"\"\n",
        "#\n",
        "#     :param t: observed treatment (ground truth)\n",
        "#     :param ps: propensity score\n",
        "#     :return: auc score\n",
        "#     \"\"\"\n",
        "#\n",
        "#     # Compute ROC curve and ROC area for each class\n",
        "#     fpr = dict()\n",
        "#     tpr = dict()\n",
        "#     roc_auc = dict()\n",
        "#     for i in range(n_classes):\n",
        "#         fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
        "#         roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "#\n",
        "#     # Compute micro-average ROC curve and ROC area\n",
        "#     fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n",
        "#     roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
        "#\n",
        "#     plt.figure()\n",
        "#     lw = 2\n",
        "#     plt.plot(fpr[2], tpr[2], color='darkorange',\n",
        "#              lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[2])\n",
        "#     plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "#     plt.xlim([0.0, 1.0])\n",
        "#     plt.ylim([0.0, 1.05])\n",
        "#     plt.xlabel('False Positive Rate')\n",
        "#     plt.ylabel('True Positive Rate')\n",
        "#     plt.title('Receiver operating characteristic example')\n",
        "#     plt.legend(loc=\"lower right\")\n",
        "    # plt.show()\n",
        "    # plt.savefig('./figs/' + name + extra_str + str(exp_id) + 'ps_dist.pdf', bbox_inches='tight')\n",
        "\n",
        "#def distance_matrix(x,y,p=2):\n",
        "#    \"\"\" Computes the squared Euclidean distance between all pairs x in X, y in Y \"\"\"\n",
        "#    x = x.squeeze()\n",
        "#    y = y.squeeze()\n",
        "#    C = -2*x.matmul(torch.t(y))\n",
        "#    nx = torch.sum(x.pow(2),dim=1).view(-1,1)\n",
        "#    ny = torch.sum(y.pow(2),dim=1).view(-1,1)\n",
        "#    D = (C + torch.t(ny)) + nx\n",
        "#    return D\n"
      ],
      "metadata": {
        "id": "B7xeBhK9z0DA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import argparse\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "# import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "# from scipy import sparse as sp\n",
        "import csv\n",
        "\n",
        "# Training settings\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--nocuda', type=int, default=0,\n",
        "                    help='Disables CUDA training.')\n",
        "parser.add_argument('--dataset', type=str, default='BlogCatalog')\n",
        "parser.add_argument('--extrastr', type=str, default='_random') # 0.5,1,2, _random\n",
        "\n",
        "parser.add_argument('--seed', type=int, default=42, help='Random seed.')\n",
        "parser.add_argument('--epochs', type=int, default=200,\n",
        "                    help='Number of epochs to train.')\n",
        "parser.add_argument('--lr', type=float, default=1e-2,\n",
        "                    help='Initial learning rate.')\n",
        "parser.add_argument('--weight_decay', type=float, default=1e-5,\n",
        "                    help='Weight decay (L2 loss on parameters).')\n",
        "parser.add_argument('--hidden', type=int, default=100,\n",
        "                    help='Number of hidden units.')\n",
        "parser.add_argument('--dropout', type=float, default=0.1,\n",
        "                    help='Dropout rate (1 - keep probability).')\n",
        "parser.add_argument('--alpha', type=float, default=1e-4,\n",
        "                    help='trade-off of representation balancing.')\n",
        "parser.add_argument('--clip', type=float, default=100.,\n",
        "                    help='gradient clipping')\n",
        "parser.add_argument('--nout', type=int, default=2)\n",
        "parser.add_argument('--nin', type=int, default=2)\n",
        "\n",
        "parser.add_argument('--tr', type=float, default=0.6)\n",
        "parser.add_argument(\n",
        "    '--path', type=str, default='/content/drive/MyDrive/Causal_network_matching/data/')\n",
        "parser.add_argument('--normy', type=int, default=1)\n",
        "\n",
        "args = parser.parse_args(args=[])\n",
        "args.cuda = not args.nocuda and torch.cuda.is_available()\n",
        "Tensor = torch.cuda.FloatTensor if args.cuda else torch.FloatTensor\n",
        "LongTensor = torch.cuda.LongTensor if args.cuda else torch.LongTensor\n",
        "\n",
        "alpha = Tensor([args.alpha])\n",
        "\n",
        "np.random.seed(args.seed)\n",
        "torch.manual_seed(args.seed)\n",
        "\n",
        "loss = torch.nn.MSELoss()\n",
        "bce_loss = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "if args.cuda:\n",
        "    torch.cuda.manual_seed(args.seed)\n",
        "    alpha = alpha.cuda()\n",
        "    loss = loss.cuda()\n",
        "    bce_loss = bce_loss.cuda()\n",
        "\n",
        "def prepare(i_exp):\n",
        "\n",
        "    # Load data and init models\n",
        "    X, A, T, Y1, Y0 = load_data(args.path, name=args.dataset, original_X=False, exp_id=str(i_exp), extra_str=args.extrastr)\n",
        "\n",
        "    n = X.shape[0]\n",
        "    n_train = int(n * args.tr)\n",
        "    n_test = int(n * 0.2)\n",
        "    # n_valid = n_test\n",
        "\n",
        "    idx = np.random.permutation(n)\n",
        "    idx_train, idx_test, idx_val = idx[:n_train], idx[n_train:n_train+n_test], idx[n_train+n_test:]\n",
        "\n",
        "    X = normalize(X) #row-normalize\n",
        "    # A = utils.normalize(A+sp.eye(n))\n",
        "\n",
        "    X = X.todense()\n",
        "    X = Tensor(X)\n",
        "\n",
        "    Y1 = Tensor(np.squeeze(Y1))\n",
        "    Y0 = Tensor(np.squeeze(Y0))\n",
        "    T = LongTensor(np.squeeze(T))\n",
        "\n",
        "    A = sparse_mx_to_torch_sparse_tensor(A,cuda=args.cuda)\n",
        "\n",
        "    # print(X.shape, Y1.shape, A.shape)\n",
        "\n",
        "    idx_train = LongTensor(idx_train)\n",
        "    idx_val = LongTensor(idx_val)\n",
        "    idx_test = LongTensor(idx_test)\n",
        "\n",
        "    # Model and optimizer\n",
        "    model = GCN_DECONF(nfeat=X.shape[1],\n",
        "                nhid=args.hidden,\n",
        "                dropout=args.dropout,n_out=args.nout,n_in=args.nin,cuda=args.cuda)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(),\n",
        "                           lr=args.lr, weight_decay=args.weight_decay)\n",
        "\n",
        "    return X, A, T, Y1, Y0, idx_train, idx_val, idx_test, model, optimizer\n",
        "\n",
        "\n",
        "def train(epoch, X, A, T, Y1, Y0, idx_train, idx_val, model, optimizer):\n",
        "    t = time.time()\n",
        "    model.train()\n",
        "#    torch.nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
        "    optimizer.zero_grad()\n",
        "    yf_pred, rep, p1 = model(X, A, T)\n",
        "    ycf_pred, _, p1 = model(X, A, 1-T)\n",
        "\n",
        "    # representation balancing, you can try different distance metrics such as MMD\n",
        "    rep_t1, rep_t0 = rep[idx_train][(T[idx_train] > 0).nonzero()], rep[idx_train][(T[idx_train] < 1).nonzero()]\n",
        "    dist, _ = wasserstein(rep_t1, rep_t0, cuda=args.cuda)\n",
        "\n",
        "    YF = torch.where(T>0,Y1,Y0)\n",
        "    # YCF = torch.where(T>0,Y0,Y1)\n",
        "\n",
        "    if args.normy:\n",
        "        # recover the normalized outcomes\n",
        "        ym, ys = torch.mean(YF[idx_train]), torch.std(YF[idx_train])\n",
        "        YFtr, YFva = (YF[idx_train] - ym) / ys, (YF[idx_val] - ym) / ys\n",
        "    else:\n",
        "        YFtr = YF[idx_train]\n",
        "        YFva = YF[idx_val]\n",
        "\n",
        "    loss_train = loss(yf_pred[idx_train], YFtr) + alpha * dist\n",
        "\n",
        "    # acc_train = accuracy(output[idx_train], labels[idx_train])\n",
        "    loss_train.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch%10==0:\n",
        "        # validation\n",
        "        loss_val = loss(yf_pred[idx_val], YFva) + alpha * dist\n",
        "\n",
        "        y1_pred, y0_pred = torch.where(T>0,yf_pred,ycf_pred), torch.where(T>0,ycf_pred,yf_pred)\n",
        "        # Y1, Y0 = torch.where(T>0, YF, YCF), torch.where(T>0, YCF, YF)\n",
        "        if args.normy:\n",
        "            y1_pred, y0_pred = y1_pred * ys + ym, y0_pred * ys + ym\n",
        "\n",
        "        # in fact, you are not supposed to do model selection w. pehe and mae_ate\n",
        "        # but it is possible to calculate with ITE ground truth (which often isn't available)\n",
        "\n",
        "        # pehe_val = torch.sqrt(loss((y1_pred - y0_pred)[idx_val],(Y1 - Y0)[idx_val]))\n",
        "        # mae_ate_val = torch.abs(\n",
        "        #     torch.mean((y1_pred - y0_pred)[idx_val])-torch.mean((Y1 - Y0)[idx_val]))\n",
        "\n",
        "        print('Epoch: {:04d}'.format(epoch+1),\n",
        "              'loss_train: {:.4f}'.format(loss_train.item()),\n",
        "              'loss_val: {:.4f}'.format(loss_val.item()),\n",
        "              # 'pehe_val: {:.4f}'.format(pehe_val.item()),\n",
        "              # 'mae_ate_val: {:.4f}'.format(mae_ate_val.item()),\n",
        "              'time: {:.4f}s'.format(time.time() - t))\n",
        "\n",
        "def eva(X, A, T, Y1, Y0, idx_train, idx_test, model, i_exp):\n",
        "    model.eval()\n",
        "    yf_pred, rep, p1 = model(X, A, T) # p1 can be used as propensity scores\n",
        "    # yf = torch.where(T>0, Y1, Y0)\n",
        "    ycf_pred, _, _ = model(X, A, 1-T)\n",
        "\n",
        "    YF = torch.where(T>0,Y1,Y0)\n",
        "    YCF = torch.where(T>0,Y0,Y1)\n",
        "\n",
        "    ym, ys = torch.mean(YF[idx_train]), torch.std(YF[idx_train])\n",
        "    # YFtr, YFva = (YF[idx_train] - ym) / ys, (YF[idx_val] - ym) / ys\n",
        "\n",
        "    y1_pred, y0_pred = torch.where(T>0,yf_pred,ycf_pred), torch.where(T>0,ycf_pred,yf_pred)\n",
        "\n",
        "    if args.normy:\n",
        "        y1_pred, y0_pred = y1_pred * ys + ym, y0_pred * ys + ym\n",
        "\n",
        "    # Y1, Y0 = torch.where(T>0, YF, YCF), torch.where(T>0, YCF, YF)\n",
        "    pehe_out = torch.sqrt(loss((y1_pred - y0_pred)[idx_test],(Y1 - Y0)[idx_test]))\n",
        "    ATE_error_out = torch.abs(torch.mean((y1_pred - y0_pred)[idx_test])-torch.mean((Y1 - Y0)[idx_test]))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    idx_train_val = torch.cat([idx_train,idx_val])\n",
        "\n",
        "    T_numpy = T.detach().cpu().numpy()\n",
        "\n",
        "    y1_pred[np.where(T_numpy == 1)] = Y1[np.where(T_numpy == 1)]\n",
        "\n",
        "    y0_pred[np.where(T_numpy == 0)] = Y0[np.where(T_numpy == 0)]\n",
        "\n",
        "\n",
        "\n",
        "    pehe_in = torch.sqrt(loss((y1_pred - y0_pred)[idx_train_val],(Y1 - Y0)[idx_train_val]))\n",
        "    ATE_error_in = torch.abs(torch.mean((y1_pred - y0_pred)[idx_train_val])-torch.mean((Y1 - Y0)[idx_train_val]))\n",
        "\n",
        "\n",
        "    pehe_out = pehe_out.item()\n",
        "    ATE_error_out = ATE_error_out.item()\n",
        "\n",
        "    pehe_in = pehe_in.item()\n",
        "    ATE_error_in = ATE_error_in.item()\n",
        "\n",
        "\n",
        "    return ATE_error_in , pehe_in, ATE_error_out, pehe_out\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    ATE_error_in_list = []\n",
        "    pehe_in_list = []\n",
        "\n",
        "    ATE_error_out_list = []\n",
        "    pehe_out_list = []\n",
        "\n",
        "    import time\n",
        "    # Record the start time\n",
        "    start_time = time.time()\n",
        "\n",
        "    for i_exp in range(0,10):\n",
        "\n",
        "        # Train model\n",
        "        X, A, T, Y1, Y0, idx_train, idx_val, idx_test, model, optimizer = prepare(i_exp)\n",
        "        t_total = time.time()\n",
        "        for epoch in range(args.epochs):\n",
        "            train(epoch, X, A, T, Y1, Y0, idx_train, idx_val, model, optimizer)\n",
        "        print(\"Optimization Finished!\")\n",
        "        print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
        "\n",
        "        # Testing\n",
        "        errors = eva(X, A, T, Y1, Y0, idx_train, idx_test, model, i_exp)\n",
        "\n",
        "        ATE_error_in_list.append(errors[0])\n",
        "        pehe_in_list.append(errors[1])\n",
        "\n",
        "        ATE_error_out_list.append(errors[2])\n",
        "        pehe_out_list.append(errors[3])\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = (end_time - start_time)/10\n",
        "\n",
        "    print(\"In-sample ATE absolute error for NetDeconf =\", round(np.mean(ATE_error_in_list),2),\"+-\", round((np.std(ATE_error_in_list, ddof=1) / np.sqrt(np.size(ATE_error_in_list))),2))\n",
        "    print(\"In-sample PEHE RMSE for NetDeconf =\", round(np.mean(pehe_in_list),2),\"+-\", round((np.std(pehe_in_list, ddof=1) / np.sqrt(np.size(pehe_in_list))),2))\n",
        "\n",
        "    print(\"Out-sample ATE error for NetDeconf =\", round(np.mean(ATE_error_out_list),2),\"+-\", round((np.std(ATE_error_out_list, ddof=1) / np.sqrt(np.size(ATE_error_out_list))),2))\n",
        "    print(\"Out-sample PEHE error for NetDeconf =\", round(np.mean(pehe_out_list),2),\"+-\", round((np.std(pehe_out_list, ddof=1) / np.sqrt(np.size(pehe_out_list))),2))\n",
        "\n",
        "    print(\" Wall time in seconds for NetDeconf for 1 simulation(avg over 10) =\", elapsed_time)"
      ],
      "metadata": {
        "id": "L3XcOZgez8xN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35ace060-52fa-45e0-ecb4-f46a08bc300d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-af30ea9765b9>:23: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:641.)\n",
            "  sparse_tensor = torch.sparse.FloatTensor(indices, values, shape)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[GraphConvolution (2195 -> 100), GraphConvolution (100 -> 100)]\n",
            "Epoch: 0001 loss_train: 4.5342 loss_val: 4.5293 time: 0.7177s\n",
            "Epoch: 0011 loss_train: 2.0742 loss_val: 1.8407 time: 0.3189s\n",
            "Epoch: 0021 loss_train: 1.4387 loss_val: 1.2785 time: 0.4158s\n",
            "Epoch: 0031 loss_train: 0.9391 loss_val: 0.8462 time: 0.4434s\n",
            "Epoch: 0041 loss_train: 0.7870 loss_val: 0.7256 time: 0.3919s\n",
            "Epoch: 0051 loss_train: 0.6108 loss_val: 0.6415 time: 0.4530s\n",
            "Epoch: 0061 loss_train: 0.5266 loss_val: 0.5641 time: 0.3684s\n",
            "Epoch: 0071 loss_train: 0.4560 loss_val: 0.4880 time: 0.3276s\n",
            "Epoch: 0081 loss_train: 0.4346 loss_val: 0.4275 time: 0.3796s\n",
            "Epoch: 0091 loss_train: 0.3661 loss_val: 0.3919 time: 0.3772s\n",
            "Epoch: 0101 loss_train: 0.3434 loss_val: 0.3540 time: 0.3340s\n",
            "Epoch: 0111 loss_train: 0.3334 loss_val: 0.3316 time: 0.3545s\n",
            "Epoch: 0121 loss_train: 0.3140 loss_val: 0.3484 time: 0.4154s\n",
            "Epoch: 0131 loss_train: 0.2906 loss_val: 0.3032 time: 0.3050s\n",
            "Epoch: 0141 loss_train: 0.2891 loss_val: 0.2923 time: 0.3202s\n",
            "Epoch: 0151 loss_train: 0.2833 loss_val: 0.2745 time: 0.3188s\n",
            "Epoch: 0161 loss_train: 0.2811 loss_val: 0.2707 time: 0.4502s\n",
            "Epoch: 0171 loss_train: 0.2678 loss_val: 0.2661 time: 0.3216s\n",
            "Epoch: 0181 loss_train: 0.2685 loss_val: 0.2642 time: 0.3058s\n",
            "Epoch: 0191 loss_train: 0.2637 loss_val: 0.2694 time: 0.3527s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 72.7074s\n",
            "[GraphConvolution (2195 -> 100), GraphConvolution (100 -> 100)]\n",
            "Epoch: 0001 loss_train: 10.7189 loss_val: 9.9879 time: 0.3104s\n",
            "Epoch: 0011 loss_train: 1.6825 loss_val: 1.4889 time: 0.3023s\n",
            "Epoch: 0021 loss_train: 1.5007 loss_val: 1.2727 time: 0.2931s\n",
            "Epoch: 0031 loss_train: 1.3183 loss_val: 1.2240 time: 0.3380s\n",
            "Epoch: 0041 loss_train: 0.9858 loss_val: 0.9932 time: 0.3423s\n",
            "Epoch: 0051 loss_train: 0.7761 loss_val: 0.7789 time: 0.3018s\n",
            "Epoch: 0061 loss_train: 0.6752 loss_val: 0.7070 time: 0.3081s\n",
            "Epoch: 0071 loss_train: 0.6508 loss_val: 0.6154 time: 0.4028s\n",
            "Epoch: 0081 loss_train: 0.5908 loss_val: 0.5689 time: 0.3023s\n",
            "Epoch: 0091 loss_train: 0.5324 loss_val: 0.5136 time: 0.3023s\n",
            "Epoch: 0101 loss_train: 0.4940 loss_val: 0.4788 time: 0.2927s\n",
            "Epoch: 0111 loss_train: 0.4758 loss_val: 0.4662 time: 0.3374s\n",
            "Epoch: 0121 loss_train: 0.4469 loss_val: 0.4403 time: 0.3073s\n",
            "Epoch: 0131 loss_train: 0.4227 loss_val: 0.4351 time: 0.3011s\n",
            "Epoch: 0141 loss_train: 0.4087 loss_val: 0.4085 time: 0.3043s\n",
            "Epoch: 0151 loss_train: 0.4075 loss_val: 0.3941 time: 0.3343s\n",
            "Epoch: 0161 loss_train: 0.4051 loss_val: 0.3921 time: 0.2938s\n",
            "Epoch: 0171 loss_train: 0.3849 loss_val: 0.3889 time: 0.2986s\n",
            "Epoch: 0181 loss_train: 0.3710 loss_val: 0.3892 time: 0.3225s\n",
            "Epoch: 0191 loss_train: 0.3770 loss_val: 0.3843 time: 0.3625s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 63.1744s\n",
            "[GraphConvolution (2195 -> 100), GraphConvolution (100 -> 100)]\n",
            "Epoch: 0001 loss_train: 3.3098 loss_val: 2.9770 time: 0.3267s\n",
            "Epoch: 0011 loss_train: 1.9882 loss_val: 1.8660 time: 0.3106s\n",
            "Epoch: 0021 loss_train: 1.2671 loss_val: 1.2085 time: 0.3326s\n",
            "Epoch: 0031 loss_train: 1.0319 loss_val: 1.0149 time: 0.4061s\n",
            "Epoch: 0041 loss_train: 0.8356 loss_val: 0.8092 time: 0.3054s\n",
            "Epoch: 0051 loss_train: 0.6948 loss_val: 0.7045 time: 0.3064s\n",
            "Epoch: 0061 loss_train: 0.6660 loss_val: 0.5998 time: 0.3083s\n",
            "Epoch: 0071 loss_train: 0.5666 loss_val: 0.5532 time: 0.3897s\n",
            "Epoch: 0081 loss_train: 0.4884 loss_val: 0.5199 time: 0.2983s\n",
            "Epoch: 0091 loss_train: 0.4750 loss_val: 0.4830 time: 0.3019s\n",
            "Epoch: 0101 loss_train: 0.4509 loss_val: 0.4531 time: 0.2945s\n",
            "Epoch: 0111 loss_train: 0.4180 loss_val: 0.4429 time: 0.4126s\n",
            "Epoch: 0121 loss_train: 0.4079 loss_val: 0.4383 time: 0.3047s\n",
            "Epoch: 0131 loss_train: 0.3963 loss_val: 0.3950 time: 0.3093s\n",
            "Epoch: 0141 loss_train: 0.3855 loss_val: 0.3951 time: 0.3119s\n",
            "Epoch: 0151 loss_train: 0.3686 loss_val: 0.3817 time: 0.3462s\n",
            "Epoch: 0161 loss_train: 0.3693 loss_val: 0.3792 time: 0.3025s\n",
            "Epoch: 0171 loss_train: 0.3596 loss_val: 0.3749 time: 0.2849s\n",
            "Epoch: 0181 loss_train: 0.3649 loss_val: 0.3667 time: 0.2867s\n",
            "Epoch: 0191 loss_train: 0.3612 loss_val: 0.3758 time: 0.3292s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 63.5271s\n",
            "[GraphConvolution (2195 -> 100), GraphConvolution (100 -> 100)]\n",
            "Epoch: 0001 loss_train: 1.6202 loss_val: 1.8391 time: 0.2972s\n",
            "Epoch: 0011 loss_train: 1.6922 loss_val: 1.8326 time: 0.2991s\n",
            "Epoch: 0021 loss_train: 1.1694 loss_val: 1.1738 time: 0.2889s\n",
            "Epoch: 0031 loss_train: 0.9148 loss_val: 1.0340 time: 0.3908s\n",
            "Epoch: 0041 loss_train: 0.7401 loss_val: 0.7413 time: 0.2884s\n",
            "Epoch: 0051 loss_train: 0.6476 loss_val: 0.6657 time: 0.2878s\n",
            "Epoch: 0061 loss_train: 0.5943 loss_val: 0.6429 time: 0.2826s\n",
            "Epoch: 0071 loss_train: 0.5517 loss_val: 0.5434 time: 0.3304s\n",
            "Epoch: 0081 loss_train: 0.5032 loss_val: 0.5440 time: 0.2878s\n",
            "Epoch: 0091 loss_train: 0.4820 loss_val: 0.4872 time: 0.2898s\n",
            "Epoch: 0101 loss_train: 0.4740 loss_val: 0.4920 time: 0.2879s\n",
            "Epoch: 0111 loss_train: 0.4559 loss_val: 0.4747 time: 0.3434s\n",
            "Epoch: 0121 loss_train: 0.4394 loss_val: 0.4616 time: 0.2752s\n",
            "Epoch: 0131 loss_train: 0.4294 loss_val: 0.4497 time: 0.2819s\n",
            "Epoch: 0141 loss_train: 0.4262 loss_val: 0.4459 time: 0.2919s\n",
            "Epoch: 0151 loss_train: 0.4134 loss_val: 0.4151 time: 0.3791s\n",
            "Epoch: 0161 loss_train: 0.4111 loss_val: 0.4289 time: 0.3043s\n",
            "Epoch: 0171 loss_train: 0.4118 loss_val: 0.4335 time: 0.2977s\n",
            "Epoch: 0181 loss_train: 0.4069 loss_val: 0.4266 time: 0.2801s\n",
            "Epoch: 0191 loss_train: 0.4051 loss_val: 0.4214 time: 0.3064s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 61.3211s\n",
            "[GraphConvolution (2195 -> 100), GraphConvolution (100 -> 100)]\n",
            "Epoch: 0001 loss_train: 5.6395 loss_val: 5.3458 time: 0.3088s\n",
            "Epoch: 0011 loss_train: 1.8650 loss_val: 2.0997 time: 0.2973s\n",
            "Epoch: 0021 loss_train: 1.0729 loss_val: 1.0622 time: 0.3102s\n",
            "Epoch: 0031 loss_train: 0.8311 loss_val: 0.8986 time: 0.4057s\n",
            "Epoch: 0041 loss_train: 0.7065 loss_val: 0.7262 time: 0.2909s\n",
            "Epoch: 0051 loss_train: 0.5631 loss_val: 0.5918 time: 0.3164s\n",
            "Epoch: 0061 loss_train: 0.5037 loss_val: 0.5480 time: 0.3073s\n",
            "Epoch: 0071 loss_train: 0.4580 loss_val: 0.4966 time: 0.3552s\n",
            "Epoch: 0081 loss_train: 0.4453 loss_val: 0.4330 time: 0.2988s\n",
            "Epoch: 0091 loss_train: 0.3934 loss_val: 0.3888 time: 0.3016s\n",
            "Epoch: 0101 loss_train: 0.3747 loss_val: 0.3770 time: 0.2974s\n",
            "Epoch: 0111 loss_train: 0.3612 loss_val: 0.3554 time: 0.3992s\n",
            "Epoch: 0121 loss_train: 0.3420 loss_val: 0.3326 time: 0.3162s\n",
            "Epoch: 0131 loss_train: 0.3313 loss_val: 0.3459 time: 0.3271s\n",
            "Epoch: 0141 loss_train: 0.3248 loss_val: 0.3430 time: 0.2949s\n",
            "Epoch: 0151 loss_train: 0.3211 loss_val: 0.3283 time: 0.3824s\n",
            "Epoch: 0161 loss_train: 0.3122 loss_val: 0.3185 time: 0.2855s\n",
            "Epoch: 0171 loss_train: 0.3142 loss_val: 0.3237 time: 0.3109s\n",
            "Epoch: 0181 loss_train: 0.3135 loss_val: 0.3262 time: 0.3126s\n",
            "Epoch: 0191 loss_train: 0.3067 loss_val: 0.3260 time: 0.4074s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 64.0484s\n",
            "[GraphConvolution (2195 -> 100), GraphConvolution (100 -> 100)]\n",
            "Epoch: 0001 loss_train: 2.6438 loss_val: 2.7500 time: 0.3397s\n",
            "Epoch: 0011 loss_train: 1.3680 loss_val: 1.4797 time: 0.2932s\n",
            "Epoch: 0021 loss_train: 0.9310 loss_val: 0.9351 time: 0.3124s\n",
            "Epoch: 0031 loss_train: 0.6422 loss_val: 0.7170 time: 0.3313s\n",
            "Epoch: 0041 loss_train: 0.5777 loss_val: 0.5975 time: 0.3124s\n",
            "Epoch: 0051 loss_train: 0.4776 loss_val: 0.5643 time: 0.2963s\n",
            "Epoch: 0061 loss_train: 0.4181 loss_val: 0.4798 time: 0.3004s\n",
            "Epoch: 0071 loss_train: 0.3970 loss_val: 0.4313 time: 0.3814s\n",
            "Epoch: 0081 loss_train: 0.3490 loss_val: 0.3775 time: 0.3162s\n",
            "Epoch: 0091 loss_train: 0.3326 loss_val: 0.3608 time: 0.3142s\n",
            "Epoch: 0101 loss_train: 0.3221 loss_val: 0.3466 time: 0.2947s\n",
            "Epoch: 0111 loss_train: 0.3109 loss_val: 0.3344 time: 0.3408s\n",
            "Epoch: 0121 loss_train: 0.3012 loss_val: 0.3422 time: 0.2978s\n",
            "Epoch: 0131 loss_train: 0.3042 loss_val: 0.3172 time: 0.3172s\n",
            "Epoch: 0141 loss_train: 0.2859 loss_val: 0.2962 time: 0.2877s\n",
            "Epoch: 0151 loss_train: 0.2847 loss_val: 0.3010 time: 0.3141s\n",
            "Epoch: 0161 loss_train: 0.2766 loss_val: 0.3057 time: 0.3027s\n",
            "Epoch: 0171 loss_train: 0.2713 loss_val: 0.2938 time: 0.3018s\n",
            "Epoch: 0181 loss_train: 0.2719 loss_val: 0.2874 time: 0.3061s\n",
            "Epoch: 0191 loss_train: 0.2744 loss_val: 0.2980 time: 0.3419s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 64.2275s\n",
            "[GraphConvolution (2195 -> 100), GraphConvolution (100 -> 100)]\n",
            "Epoch: 0001 loss_train: 25.0155 loss_val: 23.0039 time: 0.3073s\n",
            "Epoch: 0011 loss_train: 5.5310 loss_val: 5.1385 time: 0.2941s\n",
            "Epoch: 0021 loss_train: 2.1245 loss_val: 2.1014 time: 0.2960s\n",
            "Epoch: 0031 loss_train: 1.3204 loss_val: 1.2800 time: 0.3031s\n",
            "Epoch: 0041 loss_train: 1.2398 loss_val: 1.2014 time: 0.2925s\n",
            "Epoch: 0051 loss_train: 1.1505 loss_val: 0.9724 time: 0.2885s\n",
            "Epoch: 0061 loss_train: 0.9659 loss_val: 0.9038 time: 0.2946s\n",
            "Epoch: 0071 loss_train: 0.7983 loss_val: 0.7587 time: 0.3050s\n",
            "Epoch: 0081 loss_train: 0.7657 loss_val: 0.7074 time: 0.3001s\n",
            "Epoch: 0091 loss_train: 0.7049 loss_val: 0.6658 time: 0.3000s\n",
            "Epoch: 0101 loss_train: 0.6242 loss_val: 0.5864 time: 0.3164s\n",
            "Epoch: 0111 loss_train: 0.5591 loss_val: 0.5508 time: 0.3014s\n",
            "Epoch: 0121 loss_train: 0.5472 loss_val: 0.5511 time: 0.2912s\n",
            "Epoch: 0131 loss_train: 0.5317 loss_val: 0.5365 time: 0.2968s\n",
            "Epoch: 0141 loss_train: 0.4985 loss_val: 0.5171 time: 0.2952s\n",
            "Epoch: 0151 loss_train: 0.4918 loss_val: 0.4680 time: 0.3085s\n",
            "Epoch: 0161 loss_train: 0.4667 loss_val: 0.4621 time: 0.2988s\n",
            "Epoch: 0171 loss_train: 0.4529 loss_val: 0.4539 time: 0.3061s\n",
            "Epoch: 0181 loss_train: 0.4419 loss_val: 0.4383 time: 0.2941s\n",
            "Epoch: 0191 loss_train: 0.4327 loss_val: 0.4479 time: 0.2911s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 63.1064s\n",
            "[GraphConvolution (2195 -> 100), GraphConvolution (100 -> 100)]\n",
            "Epoch: 0001 loss_train: 4.1583 loss_val: 4.0015 time: 0.3112s\n",
            "Epoch: 0011 loss_train: 2.4634 loss_val: 2.1952 time: 0.2937s\n",
            "Epoch: 0021 loss_train: 1.6471 loss_val: 1.6681 time: 0.3590s\n",
            "Epoch: 0031 loss_train: 1.2145 loss_val: 1.1048 time: 0.2999s\n",
            "Epoch: 0041 loss_train: 0.9459 loss_val: 0.9428 time: 0.3040s\n",
            "Epoch: 0051 loss_train: 0.8335 loss_val: 0.7634 time: 0.2872s\n",
            "Epoch: 0061 loss_train: 0.6794 loss_val: 0.6872 time: 0.2896s\n",
            "Epoch: 0071 loss_train: 0.5975 loss_val: 0.6178 time: 0.2990s\n",
            "Epoch: 0081 loss_train: 0.5616 loss_val: 0.5587 time: 0.2986s\n",
            "Epoch: 0091 loss_train: 0.5032 loss_val: 0.5196 time: 0.2997s\n",
            "Epoch: 0101 loss_train: 0.4738 loss_val: 0.4879 time: 0.3032s\n",
            "Epoch: 0111 loss_train: 0.4514 loss_val: 0.4849 time: 0.2888s\n",
            "Epoch: 0121 loss_train: 0.4352 loss_val: 0.4568 time: 0.2920s\n",
            "Epoch: 0131 loss_train: 0.4208 loss_val: 0.4373 time: 0.2914s\n",
            "Epoch: 0141 loss_train: 0.4079 loss_val: 0.4263 time: 0.2841s\n",
            "Epoch: 0151 loss_train: 0.4014 loss_val: 0.4259 time: 0.2952s\n",
            "Epoch: 0161 loss_train: 0.3999 loss_val: 0.4066 time: 0.2793s\n",
            "Epoch: 0171 loss_train: 0.3918 loss_val: 0.4126 time: 0.2916s\n",
            "Epoch: 0181 loss_train: 0.3895 loss_val: 0.4065 time: 0.2914s\n",
            "Epoch: 0191 loss_train: 0.3816 loss_val: 0.4077 time: 0.2894s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 62.6610s\n",
            "[GraphConvolution (2195 -> 100), GraphConvolution (100 -> 100)]\n",
            "Epoch: 0001 loss_train: 5.9858 loss_val: 6.2914 time: 0.3006s\n",
            "Epoch: 0011 loss_train: 2.2039 loss_val: 2.1274 time: 0.3015s\n",
            "Epoch: 0021 loss_train: 0.9833 loss_val: 0.9609 time: 0.3023s\n",
            "Epoch: 0031 loss_train: 0.9807 loss_val: 0.9748 time: 0.3066s\n",
            "Epoch: 0041 loss_train: 0.7641 loss_val: 0.7898 time: 0.2892s\n",
            "Epoch: 0051 loss_train: 0.6913 loss_val: 0.6799 time: 0.2923s\n",
            "Epoch: 0061 loss_train: 0.6236 loss_val: 0.5961 time: 0.3028s\n",
            "Epoch: 0071 loss_train: 0.5861 loss_val: 0.5604 time: 0.2942s\n",
            "Epoch: 0081 loss_train: 0.5350 loss_val: 0.5039 time: 0.2925s\n",
            "Epoch: 0091 loss_train: 0.5142 loss_val: 0.4809 time: 0.2941s\n",
            "Epoch: 0101 loss_train: 0.4927 loss_val: 0.4575 time: 0.3031s\n",
            "Epoch: 0111 loss_train: 0.4566 loss_val: 0.4374 time: 0.2983s\n",
            "Epoch: 0121 loss_train: 0.4520 loss_val: 0.4185 time: 0.2994s\n",
            "Epoch: 0131 loss_train: 0.4451 loss_val: 0.4173 time: 0.2803s\n",
            "Epoch: 0141 loss_train: 0.4353 loss_val: 0.4082 time: 0.3114s\n",
            "Epoch: 0151 loss_train: 0.4278 loss_val: 0.3972 time: 0.3373s\n",
            "Epoch: 0161 loss_train: 0.4118 loss_val: 0.4027 time: 0.2874s\n",
            "Epoch: 0171 loss_train: 0.4070 loss_val: 0.3801 time: 0.2963s\n",
            "Epoch: 0181 loss_train: 0.4072 loss_val: 0.3839 time: 0.2962s\n",
            "Epoch: 0191 loss_train: 0.3981 loss_val: 0.3867 time: 0.3344s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 61.6936s\n",
            "[GraphConvolution (2195 -> 100), GraphConvolution (100 -> 100)]\n",
            "Epoch: 0001 loss_train: 6.3651 loss_val: 7.4766 time: 0.3156s\n",
            "Epoch: 0011 loss_train: 1.9314 loss_val: 2.0202 time: 0.3084s\n",
            "Epoch: 0021 loss_train: 1.5909 loss_val: 1.5043 time: 0.3153s\n",
            "Epoch: 0031 loss_train: 0.9148 loss_val: 0.9502 time: 0.3006s\n",
            "Epoch: 0041 loss_train: 0.7105 loss_val: 0.6906 time: 0.3012s\n",
            "Epoch: 0051 loss_train: 0.6128 loss_val: 0.6247 time: 0.3196s\n",
            "Epoch: 0061 loss_train: 0.4855 loss_val: 0.4974 time: 0.3140s\n",
            "Epoch: 0071 loss_train: 0.4498 loss_val: 0.4246 time: 0.3035s\n",
            "Epoch: 0081 loss_train: 0.3880 loss_val: 0.3872 time: 0.3048s\n",
            "Epoch: 0091 loss_train: 0.3635 loss_val: 0.3841 time: 0.3014s\n",
            "Epoch: 0101 loss_train: 0.3317 loss_val: 0.3589 time: 0.4023s\n",
            "Epoch: 0111 loss_train: 0.3213 loss_val: 0.3213 time: 0.3100s\n",
            "Epoch: 0121 loss_train: 0.3069 loss_val: 0.3182 time: 0.3239s\n",
            "Epoch: 0131 loss_train: 0.2884 loss_val: 0.3035 time: 0.3354s\n",
            "Epoch: 0141 loss_train: 0.2802 loss_val: 0.2864 time: 0.3582s\n",
            "Epoch: 0151 loss_train: 0.2805 loss_val: 0.2795 time: 0.3203s\n",
            "Epoch: 0161 loss_train: 0.2689 loss_val: 0.2708 time: 0.3064s\n",
            "Epoch: 0171 loss_train: 0.2671 loss_val: 0.2673 time: 0.2974s\n",
            "Epoch: 0181 loss_train: 0.2615 loss_val: 0.2638 time: 0.4203s\n",
            "Epoch: 0191 loss_train: 0.2605 loss_val: 0.2650 time: 0.3012s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 66.4187s\n",
            "In-sample ATE absolute error for NetDeconf = 0.31 +- 0.05\n",
            "In-sample PEHE RMSE for NetDeconf = 1.94 +- 0.01\n",
            "Out-sample ATE error for NetDeconf = 0.33 +- 0.07\n",
            "Out-sample PEHE error for NetDeconf = 1.08 +- 0.06\n",
            " Wall time in seconds for NetDeconf for 1 simulation(avg over 10) = 65.01690742969512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HcRGAw88rwQW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}