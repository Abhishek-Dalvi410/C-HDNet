{"cells":[{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":388,"status":"ok","timestamp":1726347051638,"user":{"displayName":"Abhishek Dalvi","userId":"01466421205583090743"},"user_tz":240},"id":"jmIskjM3u2mW"},"outputs":[],"source":["import numpy as np\n","import sys\n","import tensorflow as tf\n","from keras.losses import Loss\n","from sklearn.preprocessing import StandardScaler\n","from scipy.sparse import csr_matrix\n","import pandas as pd\n","import datetime\n","import time\n","import io\n","from sklearn.model_selection import train_test_split\n","import scipy.io as sio"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1726347051998,"user":{"displayName":"Abhishek Dalvi","userId":"01466421205583090743"},"user_tz":240},"id":"z2mcUbsxAUdJ"},"outputs":[],"source":["def load_data(dataset, kappa, exp_id, original_X=False, extra_str=\"\"):\n","    path = '/content/drive/MyDrive/Causal_network_matching/data/'+dataset+str(kappa)+'/'+str(dataset)+''+str(exp_id)+'.mat'\n","    print(path)\n","    data = sio.loadmat(path)\n","    A = data['Network']  # csr matrix\n","\n","    if not original_X:\n","        X = data['X_100']\n","    else:\n","        X = data['Attributes']\n","\n","    mu_1 = data['Y1']\n","    mu_0 = data['Y0']\n","    T = data['T']\n","\n","    T = T.flatten()\n","    mu_1 = mu_1.flatten()\n","    mu_0 = mu_0.flatten()\n","\n","    Y_observed = []\n","    for i in range(len(T)):\n","        if T[i] == 1:\n","            Y_observed.append(mu_1[i])\n","        else:\n","            Y_observed.append(mu_0[i])\n","\n","    Y_observed = np.array(Y_observed)\n","    X = X.todense()\n","\n","\n","    X_train, X_test, Y_factual_train, _, T_train, _, mu_0_train, mu_0_test, mu_1_train, mu_1_test = train_test_split(X, Y_observed, T, mu_0, mu_1, test_size=0.2)\n","\n","    X_train = X_train.astype('float32')\n","    Y_factual_train = Y_factual_train.astype('float32') #most GPUs only compute 32-bit floats\n","    T_train = T_train.astype('float32')\n","    mu_0_train = mu_0_train.astype('float32')\n","    mu_1_train = mu_1_train.astype('float32')\n","\n","    X_test = X_test.astype('float32')\n","    mu_0_test = mu_0_test.astype('float32')\n","    mu_1_test = mu_1_test.astype('float32')\n","\n","    data_train={'x':X_train,'y':Y_factual_train,'t':T_train,'mu_0':mu_0_train,'mu_1':mu_1_train}\n","    data_train['t']=data_train['t'].reshape(-1,1) #we're just padding one dimensional vectors with an additional dimension\n","    data_train['y']=data_train['y'].reshape(-1,1)\n","    data_train['mu_0']=data_train['mu_0'].reshape(-1,1) #we're just padding one dimensional vectors with an additional dimension\n","    data_train['mu_1']=data_train['mu_1'].reshape(-1,1)\n","\n","    #rescaling y between 0 and 1 often makes training of DL regressors easier\n","    data_train['y_scaler'] = StandardScaler().fit(data_train['y'])\n","    data_train['ys'] = data_train['y_scaler'].transform(data_train['y'])\n","\n","    data_test={'x':X_test,'mu_0':mu_0_test,'mu_1':mu_1_test}\n","    data_test['y_scaler'] = data_train['y_scaler']\n","    data_test['mu_0']=data_test['mu_0'].reshape(-1,1) #we're just padding one dimensional vectors with an additional dimension\n","    data_test['mu_1']=data_test['mu_1'].reshape(-1,1)\n","\n","    return data_train, data_test"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1726347051998,"user":{"displayName":"Abhishek Dalvi","userId":"01466421205583090743"},"user_tz":240},"id":"rda4wsQmBWWf"},"outputs":[],"source":["def pdist2sq(x,y):\n","    x2 = tf.reduce_sum(x ** 2, axis=-1, keepdims=True)\n","    y2 = tf.reduce_sum(y ** 2, axis=-1, keepdims=True)\n","    dist = x2 + tf.transpose(y2, (1, 0)) - 2. * x @ tf.transpose(y, (1, 0))\n","    return dist\n","\n","class CFRNet_Loss(Loss):\n","  #initialize instance attributes\n","  def __init__(self, alpha=1.,sigma=1.):\n","      super().__init__()\n","      self.alpha = alpha # balances regression loss and MMD IPM\n","      self.rbf_sigma=sigma #for gaussian kernel\n","      self.name='cfrnet_loss'\n","\n","  def split_pred(self,concat_pred):\n","      #generic helper to make sure we dont make mistakes\n","      preds={}\n","      preds['y0_pred'] = concat_pred[:, 0]\n","      preds['y1_pred'] = concat_pred[:, 1]\n","      preds['phi'] = concat_pred[:, 2:]\n","      return preds\n","\n","  def rbf_kernel(self, x, y):\n","    return tf.exp(-pdist2sq(x,y)/tf.square(self.rbf_sigma))\n","\n","  def calc_mmdsq(self, Phi, t):\n","    Phic, Phit =tf.dynamic_partition(Phi,tf.cast(tf.squeeze(t),tf.int32),2)\n","\n","    Kcc = self.rbf_kernel(Phic,Phic)\n","    Kct = self.rbf_kernel(Phic,Phit)\n","    Ktt = self.rbf_kernel(Phit,Phit)\n","\n","    m = tf.cast(tf.shape(Phic)[0],Phi.dtype)\n","    n = tf.cast(tf.shape(Phit)[0],Phi.dtype)\n","\n","    mmd = 1.0/(m*(m-1.0))*(tf.reduce_sum(Kcc))\n","    mmd = mmd + 1.0/(n*(n-1.0))*(tf.reduce_sum(Ktt))\n","    mmd = mmd - 2.0/(m*n)*tf.reduce_sum(Kct)\n","    return mmd * tf.ones_like(t)\n","\n","  def mmdsq_loss(self, concat_true,concat_pred):\n","    t_true = concat_true[:, 1]\n","    p=self.split_pred(concat_pred)\n","    mmdsq_loss = tf.reduce_mean(self.calc_mmdsq(p['phi'],t_true))\n","    return mmdsq_loss\n","\n","  def regression_loss(self,concat_true,concat_pred):\n","      y_true = concat_true[:, 0]\n","      t_true = concat_true[:, 1]\n","      p = self.split_pred(concat_pred)\n","      loss0 = tf.reduce_mean((1. - t_true) * tf.square(y_true - p['y0_pred']))\n","      loss1 = tf.reduce_mean(t_true * tf.square(y_true - p['y1_pred']))\n","      return loss0+loss1\n","\n","  def cfr_loss(self,concat_true,concat_pred):\n","      lossR = self.regression_loss(concat_true,concat_pred)\n","      lossIPM = self.mmdsq_loss(concat_true,concat_pred)\n","      return lossR + self.alpha * lossIPM\n","\n","      #return lossR + self.alpha * lossIPM\n","\n","  #compute loss\n","  def call(self, concat_true, concat_pred):\n","      return self.cfr_loss(concat_true,concat_pred)"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1726347051998,"user":{"displayName":"Abhishek Dalvi","userId":"01466421205583090743"},"user_tz":240},"id":"AqNOvhqMBfAh"},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np\n","\n","from keras.layers import Input\n","from keras.layers import Dense\n","from keras.layers import Concatenate\n","from keras import regularizers\n","from keras import Model\n","from keras.losses import binary_crossentropy\n","from keras.metrics import binary_accuracy\n","from keras.losses import Loss\n","\n","def make_tarnet(input_dim, reg_l2):\n","\n","    x = Input(shape=(input_dim,), name='input')\n","\n","    # representation\n","    phi = Dense(units=512, activation='elu', kernel_initializer='RandomNormal',name='phi_1')(x)\n","    phi = Dense(units=256, activation='elu', kernel_initializer='RandomNormal',name='phi_2')(phi)\n","\n","    # HYPOTHESIS\n","    y0_hidden = Dense(units=256, activation='elu', kernel_regularizer=regularizers.l2(reg_l2),name='y0_hidden_1')(phi)\n","    y1_hidden = Dense(units=128, activation='elu', kernel_regularizer=regularizers.l2(reg_l2),name='y1_hidden_1')(phi)\n","\n","    # second layer\n","    y0_hidden = Dense(units=256, activation='elu', kernel_regularizer=regularizers.l2(reg_l2),name='y0_hidden_2')(y0_hidden)\n","    y1_hidden = Dense(units=128, activation='elu', kernel_regularizer=regularizers.l2(reg_l2),name='y1_hidden_2')(y1_hidden)\n","\n","    # third\n","    y0_predictions = Dense(units=1, activation=None, kernel_regularizer=regularizers.l2(reg_l2), name='y0_predictions')(y0_hidden)\n","    y1_predictions = Dense(units=1, activation=None, kernel_regularizer=regularizers.l2(reg_l2), name='y1_predictions')(y1_hidden)\n","\n","    concat_pred = Concatenate(1)([y0_predictions, y1_predictions,phi])\n","    model = Model(inputs=x, outputs=concat_pred)\n","\n","    return model"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1726347051998,"user":{"displayName":"Abhishek Dalvi","userId":"01466421205583090743"},"user_tz":240},"id":"lthGoT7vBlIe"},"outputs":[],"source":["#https://towardsdatascience.com/implementing-macro-f1-score-in-keras-what-not-to-do-e9f1aa04029d\n","class Eval_metrics_train():\n","    def __init__(self,data):\n","        self.data=data #feed the callback the full dataset\n","        #needed for PEHEnn; Called in self.find_ynn\n","        self.data['o_idx']=tf.range(self.data['t'].shape[0])\n","        self.data['c_idx']=self.data['o_idx'][self.data['t'].squeeze()==0] #These are the indices of the control units\n","        self.data['t_idx']=self.data['o_idx'][self.data['t'].squeeze()==1] #These are the indices of the treated units\n","\n","    def split_pred(self,concat_pred):\n","        preds={}\n","        preds['y0_pred'] = self.data['y_scaler'].inverse_transform(concat_pred[:, 0].reshape(-1, 1))\n","        preds['y1_pred'] = self.data['y_scaler'].inverse_transform(concat_pred[:, 1].reshape(-1, 1))\n","        preds['t_pred'] = concat_pred[:, 2]\n","        preds['epsilon'] = concat_pred[:, 3]\n","        preds['phi'] = concat_pred[:, 4:]\n","        return preds\n","\n","    def ATE_absolute_error(self,concat_pred):\n","        p = self.split_pred(concat_pred)\n","        ATT_pred = tf.gather(params=self.data['y'], indices=self.data['t_idx']) - tf.gather(params=p['y0_pred'], indices=self.data['t_idx'])\n","        ATU_pred = tf.gather(params=p['y1_pred'], indices=self.data['c_idx']) - tf.gather(params=self.data['y'], indices=self.data['c_idx'])\n","        ATE_pred = tf.reduce_mean(tf.concat([ATT_pred,ATU_pred], axis=0)) #stitch em back up!\n","        ATE_actual = tf.reduce_mean(self.data['mu_1']-self.data['mu_0'])\n","        return tf.abs(ATE_actual- ATE_pred)\n","\n","    def ITE_RMSE_error(self,concat_pred):\n","        #simulation only\n","        p = self.split_pred(concat_pred)\n","        y_1_treated_group = tf.gather(params=self.data['y'], indices=self.data['t_idx'])\n","        y_0_treated_group = tf.gather(params=p['y0_pred'], indices=self.data['t_idx'])\n","\n","        mu_1_treated_group = tf.gather(params=self.data['mu_1'], indices=self.data['t_idx'])\n","        mu_0_treated_group = tf.gather(params=self.data['mu_0'], indices=self.data['t_idx'])\n","\n","\n","        treat_grp_error = (y_1_treated_group - y_0_treated_group) - (mu_1_treated_group - mu_0_treated_group)\n","\n","        y_1_control_group = tf.gather(params=p['y1_pred'], indices=self.data['c_idx'])\n","        y_0_control_group = tf.gather(params=self.data['y'], indices=self.data['c_idx'])\n","\n","        mu_1_control_group = tf.gather(params=self.data['mu_1'], indices=self.data['c_idx'])\n","        mu_0_control_group = tf.gather(params=self.data['mu_0'], indices=self.data['c_idx'])\n","\n","        control_grp_error = (y_1_control_group - y_0_control_group) - (mu_1_control_group - mu_0_control_group)\n","\n","\n","        ITE_error = tf.concat([treat_grp_error, control_grp_error], axis=0)\n","        ITE_RMSE_error = tf.sqrt(tf.reduce_mean(tf.square(ITE_error)))\n","        return ITE_RMSE_error"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1726347051998,"user":{"displayName":"Abhishek Dalvi","userId":"01466421205583090743"},"user_tz":240},"id":"ZGXmPoNgG7x5"},"outputs":[],"source":["#https://towardsdatascience.com/implementing-macro-f1-score-in-keras-what-not-to-do-e9f1aa04029d\n","class Eval_metrics_test():\n","    def __init__(self,data):\n","        self.data=data #feed the callback the full dataset\n","\n","    def split_pred(self,concat_pred):\n","        preds={}\n","        preds['y0_pred'] = self.data['y_scaler'].inverse_transform(concat_pred[:, 0].reshape(-1, 1))\n","        preds['y1_pred'] = self.data['y_scaler'].inverse_transform(concat_pred[:, 1].reshape(-1, 1))\n","        preds['t_pred'] = concat_pred[:, 2]\n","        preds['epsilon'] = concat_pred[:, 3]\n","        preds['phi'] = concat_pred[:, 4:]\n","        return preds\n","\n","\n","    def PEHE(self,concat_pred):\n","        #simulation only\n","        p = self.split_pred(concat_pred)\n","        cate_err=tf.reduce_mean( tf.square( ( (self.data['mu_1']-self.data['mu_0']) - (p['y1_pred']-p['y0_pred']) ) ) )\n","        return tf.sqrt(cate_err)\n","\n","    def ATE_absolute_error_outsample(self,concat_pred):\n","        #simulation only\n","        p = self.split_pred(concat_pred)\n","        ATE_actual = tf.reduce_mean(self.data['mu_1']-self.data['mu_0'])\n","        ATE_pred = tf.reduce_mean(p['y1_pred']-p['y0_pred'])\n","        return tf.abs(ATE_actual- ATE_pred)"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1726347051998,"user":{"displayName":"Abhishek Dalvi","userId":"01466421205583090743"},"user_tz":240},"id":"Nf4CAA3yHzvI"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AQvXWCRpBqks","executionInfo":{"status":"ok","timestamp":1726349755724,"user_tz":240,"elapsed":311838,"user":{"displayName":"Abhishek Dalvi","userId":"01466421205583090743"}},"outputId":"96c3fbc7-535b-43d2-f897-8b22167213a3"},"outputs":[{"output_type":"stream","name":"stdout","text":["10\n","/content/drive/MyDrive/Causal_network_matching/data/Flickr2/Flickr9.mat\n","\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n"]}],"source":["from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau, TerminateOnNaN\n","from tensorflow.keras.optimizers import SGD, Adam\n","import io\n","from IPython.display import display, clear_output\n","#Colab command to allow us to run Colab in TF2\n","%load_ext tensorboard\n","# data=get_news_data(1)\n","val_split=0.20\n","batch_size=4000\n","verbose=1\n","i = 0\n","tf.random.set_seed(i)\n","np.random.seed(i)\n","!rm -rf ./logs_CFRnet/\n","sim_evals = []\n","for j in range(0,10):\n","    clear_output(wait=True)\n","    print(j+1)\n","    data_train, data_test = load_data(dataset=\"Flickr\", kappa=2, exp_id=j)\n","    yt = np.concatenate([data_train['ys'], data_train['t']], 1)\n","\n","    # Clear any logs from previous runs\n","    start_time = time.time()\n","    log_dir = \"logs_CFRnet/fit/\" +str(j)+\"/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","    file_writer = tf.summary.create_file_writer(log_dir + \"/metrics\")\n","    file_writer.set_as_default()\n","    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=0)\n","\n","    adam_callbacks = [\n","        TerminateOnNaN(),\n","        EarlyStopping(monitor='val_loss', patience=50, min_delta=0.),\n","        ReduceLROnPlateau(monitor='loss', factor=0.5, patience=100, verbose=verbose, mode='auto',\n","                          min_delta=1e-8, cooldown=0, min_lr=0),\n","        tensorboard_callback\n","    ]\n","\n","\n","    cfrnet_model=make_tarnet(data_train['x'].shape[1],.01)\n","    cfrnet_loss=CFRNet_Loss(alpha=1.0)\n","\n","    # Flickr\n","    cfrnet_model.compile(optimizer=Adam(learning_rate=0.001),\n","                         loss=cfrnet_loss,\n","                         metrics=[cfrnet_loss,cfrnet_loss.regression_loss,cfrnet_loss.mmdsq_loss])\n","\n","    # # BlogCatalog\n","    # cfrnet_model.compile(optimizer=Adam(learning_rate=0.005),\n","    #                      loss=cfrnet_loss,\n","    #                      metrics=[cfrnet_loss,cfrnet_loss.regression_loss,cfrnet_loss.mmdsq_loss])\n","\n","    cfrnet_model.fit(x=data_train['x'],y=yt,\n","                     callbacks=adam_callbacks,\n","                     validation_split=val_split,\n","                     epochs=10000,\n","                     batch_size=batch_size,\n","                     verbose=0)\n","\n","    end_time = time.time()\n","\n","    elapsed_time_seconds = end_time - start_time\n","    # Convert elapsed time to minutes\n","    elapsed_time_minutes = elapsed_time_seconds / 60\n","\n","    Evaluation_metrics_insample = Eval_metrics_train(data_train)\n","    concat_pred_insample = cfrnet_model.predict(data_train['x'])\n","    ATE_abs_insample = Evaluation_metrics_insample.ATE_absolute_error(concat_pred_insample)\n","    ITE_RMSE_insample = Evaluation_metrics_insample.ITE_RMSE_error(concat_pred_insample)\n","\n","    Evaluation_metrics_outsample = Eval_metrics_test(data_test)\n","    concat_pred_outsample = cfrnet_model.predict(data_test['x'])\n","    ATE_abs_outsample = Evaluation_metrics_outsample.ATE_absolute_error_outsample(concat_pred_outsample)\n","    PEHE_outsample = Evaluation_metrics_outsample.PEHE(concat_pred_outsample)\n","\n","    metrics = [ATE_abs_insample.numpy(), ITE_RMSE_insample.numpy(), ATE_abs_outsample.numpy(), PEHE_outsample.numpy(), elapsed_time_minutes]\n","    sim_evals.append(metrics)\n"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-a6AEuTkCMTv","executionInfo":{"status":"ok","timestamp":1726349755725,"user_tz":240,"elapsed":11,"user":{"displayName":"Abhishek Dalvi","userId":"01466421205583090743"}},"outputId":"6318ab63-aa8e-4535-934d-a6e713917335"},"outputs":[{"output_type":"stream","name":"stdout","text":[" Insample ATE error for CFR-net(mean over 10) = 6.73 +- 1.36\n"," Insample ITE_RMSE error for CFR-net(mean over 10) = 75.01 +- 4.11\n"," Outsample ATE error for CFR-net(mean over 10) = 8.45 +- 2.05\n"," Outsample PEHE error for CFR-net(mean over 10) = 93.04 +- 7.07\n"," Wall time for CFR-net(sum over 10) = 4.911457216739654\n"]}],"source":["sim_evals = np.asarray(sim_evals)\n","\n","print(\" Insample ATE error for CFR-net(mean over 10) =\", round(np.mean(sim_evals[:,0]),2),\"+-\",round((np.std(sim_evals[:,0], ddof=1) / np.sqrt(np.size(sim_evals[:,0]))),2))\n","print(\" Insample ITE_RMSE error for CFR-net(mean over 10) =\", round(np.mean(sim_evals[:,1]),2),\"+-\",round((np.std(sim_evals[:,1], ddof=1) / np.sqrt(np.size(sim_evals[:,1]))),2))\n","print(\" Outsample ATE error for CFR-net(mean over 10) =\", round(np.mean(sim_evals[:,2]),2),\"+-\",round((np.std(sim_evals[:,2], ddof=1) / np.sqrt(np.size(sim_evals[:,2]))),2))\n","print(\" Outsample PEHE error for CFR-net(mean over 10) =\", round(np.mean(sim_evals[:,3]),2),\"+-\",round((np.std(sim_evals[:,3], ddof=1) / np.sqrt(np.size(sim_evals[:,3]))),2))\n","print(\" Wall time for CFR-net(sum over 10) =\", np.sum(sim_evals[:,4]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MKuky5_pObXV"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[],"mount_file_id":"12dD2W0uwXTTOVRpPzzx-7gCx2FkJfdMV","authorship_tag":"ABX9TyMgk8AEbvp44H242bbUr1ll"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}