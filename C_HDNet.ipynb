{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mau9qgoaRMcu"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import torch\n",
        "import scipy\n",
        "import numpy as np\n",
        "import scipy.io as sio\n",
        "import scipy.sparse as sp\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.metrics import pairwise_distances\n",
        "from scipy.sparse import coo_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APrawwUfkGcC"
      },
      "outputs": [],
      "source": [
        "def load_data(dataset, kappa, exp_id, original_X=False, extra_str=\"\"):\n",
        "    path = '/content/drive/MyDrive/Causal_network_matching/data/'+dataset+str(kappa)+'/'+str(dataset)+''+str(exp_id)+'.mat'\n",
        "    print(path)\n",
        "    data = sio.loadmat(path)\n",
        "    A = data['Network']  # csr matrix\n",
        "\n",
        "    if not original_X:\n",
        "        X = data['X_100']\n",
        "    else:\n",
        "        X = data['Attributes']\n",
        "\n",
        "    mu_1 = data['Y1']\n",
        "    mu_0 = data['Y0']\n",
        "    T = data['T']\n",
        "\n",
        "\n",
        "    T = T[0]\n",
        "    mu_1 = mu_1[0]\n",
        "    mu_0 = mu_0[0]\n",
        "\n",
        "    Y_observed = np.where(T > 0, mu_1, mu_0)\n",
        "\n",
        "    X = X.todense()\n",
        "    X = np.array(X, dtype=np.float32)\n",
        "    A = A.todense()\n",
        "    A = np.array(A, dtype=np.float32)\n",
        "\n",
        "    row_sums = X.sum(axis=1)\n",
        "    row_sums[row_sums == 0.0] = 1.0\n",
        "    X = X / row_sums[:, np.newaxis]\n",
        "\n",
        "    return X, A, T, Y_observed, mu_1, mu_0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktuPjeZfT7A6"
      },
      "outputs": [],
      "source": [
        "def create_hash(features, hash_length):\n",
        "  random_A = torch.randn(features.size(1), hash_length)\n",
        "  r = torch.sparse.mm(features, random_A)\n",
        "  r = (r > 0).float()\n",
        "  r = convert_binary_to_bipolar(r)\n",
        "  return r\n",
        "\n",
        "def convert_binary_to_bipolar(HD_vecs):\n",
        "  return (2 * HD_vecs) -1\n",
        "\n",
        "def get_separate_treated_and_control(Q,T):\n",
        "  Q_treated = Q[T.astype(bool)]\n",
        "  Q_control = Q[np.logical_not(T.astype(bool))]\n",
        "  return Q_treated, Q_control"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HdbbyUPH4-Em"
      },
      "outputs": [],
      "source": [
        "def get_latent_HD_reprensentation(X,A, hash_dim):\n",
        "  X = torch.from_numpy(X)\n",
        "  A = torch.from_numpy(A)\n",
        "  adj_sparse_torch = A.to_sparse()\n",
        "\n",
        "  N = create_hash(X.to_sparse(), hash_length=hash_dim)\n",
        "  H_1 = torch.sparse.mm(adj_sparse_torch, N)\n",
        "  H_2 = torch.sparse.mm(adj_sparse_torch, H_1)\n",
        "\n",
        "  phi_0 = torch.randint(0, 2, size=(1,hash_dim))\n",
        "  phi_0 = convert_binary_to_bipolar(phi_0)\n",
        "  phi_0 = torch.tile(phi_0, (H_1.size()[0], 1))\n",
        "\n",
        "  phi_1 = torch.randint(0, 2, size=(1,hash_dim))\n",
        "  phi_1 = convert_binary_to_bipolar(phi_1)\n",
        "  phi_1 = torch.tile(phi_1, (H_1.size()[0], 1))\n",
        "\n",
        "  phi_2 = torch.randint(0, 2, size=(1,hash_dim))\n",
        "  phi_2 = convert_binary_to_bipolar(phi_2)\n",
        "  phi_2 = torch.tile(phi_2, (H_1.size()[0], 1))\n",
        "\n",
        "  Z = torch.mul(N, phi_0) + torch.mul(H_1, phi_1) + torch.mul(H_2, phi_2)\n",
        "  Z = np.array(Z, dtype=np.float32)\n",
        "\n",
        "  return Z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTDs_Z9g4_h0"
      },
      "outputs": [],
      "source": [
        "def do_matching(covariates, treatment_assignment, outcomes):\n",
        "\n",
        "  # print(covariates.shape)\n",
        "  # print(treatment_assignment.shape)\n",
        "  # print(outcomes.shape)\n",
        "  # print(\"--------------\")\n",
        "\n",
        "  X_trtd, X_cntrl = get_separate_treated_and_control(covariates, treatment_assignment)\n",
        "  Y_trtd, Y_cntrl = get_separate_treated_and_control(outcomes, treatment_assignment)\n",
        "\n",
        "  hash_length = X.shape[1]\n",
        "\n",
        "  from sklearn.neighbors import KNeighborsRegressor\n",
        "  y1_predictor = KNeighborsRegressor(n_neighbors=5, weights='distance')\n",
        "  y1_predictor.fit(X_trtd, Y_trtd)\n",
        "\n",
        "  y0_predictor = KNeighborsRegressor(n_neighbors=5, weights='distance')\n",
        "  y0_predictor.fit(X_cntrl, Y_cntrl)\n",
        "\n",
        "\n",
        "  return y1_predictor, y0_predictor\n",
        "\n",
        "\n",
        "def evaluate(Y1_predictions, Y0_predictions, mu_1_actual, mu_0_actual):\n",
        "\n",
        "  ATE_error = np.abs(np.mean(Y1_predictions-Y0_predictions) - np.mean(mu_1_actual-mu_0_actual))\n",
        "  ITE_RMSE = np.sqrt(np.mean(np.square((Y1_predictions-Y0_predictions) - (mu_1_actual - mu_0_actual))))\n",
        "\n",
        "  return ITE_RMSE, ATE_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_lVBcPPCJW8",
        "outputId": "f3bd7291-e395-45ad-d02b-6c9a04178ae7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Causal_network_matching/data/BlogCatalog_random/BlogCatalog0.mat\n",
            "(5196, 2182)\n",
            "/content/drive/MyDrive/Causal_network_matching/data/BlogCatalog_random/BlogCatalog1.mat\n",
            "(5196, 2182)\n",
            "/content/drive/MyDrive/Causal_network_matching/data/BlogCatalog_random/BlogCatalog2.mat\n",
            "(5196, 2182)\n",
            "/content/drive/MyDrive/Causal_network_matching/data/BlogCatalog_random/BlogCatalog3.mat\n",
            "(5196, 2182)\n",
            "/content/drive/MyDrive/Causal_network_matching/data/BlogCatalog_random/BlogCatalog4.mat\n",
            "(5196, 2182)\n",
            "/content/drive/MyDrive/Causal_network_matching/data/BlogCatalog_random/BlogCatalog5.mat\n",
            "(5196, 2182)\n",
            "/content/drive/MyDrive/Causal_network_matching/data/BlogCatalog_random/BlogCatalog6.mat\n",
            "(5196, 2182)\n",
            "/content/drive/MyDrive/Causal_network_matching/data/BlogCatalog_random/BlogCatalog7.mat\n",
            "(5196, 2182)\n",
            "/content/drive/MyDrive/Causal_network_matching/data/BlogCatalog_random/BlogCatalog8.mat\n",
            "(5196, 2182)\n",
            "/content/drive/MyDrive/Causal_network_matching/data/BlogCatalog_random/BlogCatalog9.mat\n",
            "(5196, 2182)\n"
          ]
        }
      ],
      "source": [
        "PEHE_in_list = []\n",
        "ATE_in_list = []\n",
        "\n",
        "PEHE_out_list = []\n",
        "ATE_out_list = []\n",
        "\n",
        "elapsed_time_list = []\n",
        "sim_evals = []\n",
        "\n",
        "import time\n",
        "\n",
        "# Record the start time\n",
        "start_time = time.time()\n",
        "for i in range(10):\n",
        "  X, A, T, Y_observed, mu_1, mu_0 = load_data(dataset=\"BlogCatalog\", kappa=\"_random\", exp_id=i)\n",
        "\n",
        "\n",
        "  # print(T.shape)\n",
        "  # print(np.sum(T))\n",
        "\n",
        "  n = X.shape[0]\n",
        "  n_train = int(n * 0.8)\n",
        "\n",
        "  # print(n)\n",
        "  # print(n_train)\n",
        "\n",
        "  idx = np.random.permutation(n)\n",
        "  idx_train, idx_test = idx[:n_train], idx[n_train:]\n",
        "  print(X.shape)\n",
        "\n",
        "  Z = get_latent_HD_reprensentation(X,A, hash_dim=10000)\n",
        "\n",
        "\n",
        "\n",
        "  Y1_model, Y0_model = do_matching(Z[idx_train], T[idx_train], Y_observed[idx_train])\n",
        "\n",
        "  Y_1pred = Y1_model.predict(Z)\n",
        "  Y_0pred = Y0_model.predict(Z)\n",
        "\n",
        "  # print(Y_1pred[idx_train].shape)\n",
        "  # print(Y_0pred[idx_train].shape)\n",
        "  # print(mu_1[idx_train].shape)\n",
        "  # print(mu_0[idx_train].shape)\n",
        "\n",
        "  PEHE_in, ATE_in = evaluate(Y_1pred[idx_train], Y_0pred[idx_train], mu_1[idx_train], mu_0[idx_train])\n",
        "\n",
        "  PEHE_out, ATE_out = evaluate(Y_1pred[idx_test], Y_0pred[idx_test], mu_1[idx_test], mu_0[idx_test])\n",
        "\n",
        "  PEHE_in_list.append(PEHE_in)\n",
        "  ATE_in_list.append(ATE_in)\n",
        "\n",
        "  PEHE_out_list.append(PEHE_out)\n",
        "  ATE_out_list.append(ATE_out)\n",
        "\n",
        "end_time = time.time()\n",
        "elapsed_time = (end_time - start_time)/10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxOLy53NDmWV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96b406bf-837b-452b-9881-6651f72b9d26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Insample ATE error for HD-net(mean over 10) = 0.27 +- 0.06\n",
            " Insample ITE_RMSE error for HD-net(mean over 10) = 1.87 +- 0.03\n",
            " Outsample ATE error for HD-net(mean over 10) = 0.2 +- 0.05\n",
            " Outsample PEHE error for HD-net(mean over 10) = 1.73 +- 0.14\n",
            " Wall time in seconds for HD-net for 1 simulation(avg over 10) = 7.817298746109008\n"
          ]
        }
      ],
      "source": [
        "print(\" Insample ATE error for HD-net(mean over 10) =\", round(np.mean(ATE_in_list),2),\"+-\",round((np.std(ATE_in_list, ddof=1) / np.sqrt(np.size(ATE_in_list))),2))\n",
        "print(\" Insample ITE_RMSE error for HD-net(mean over 10) =\", round(np.mean(PEHE_in_list),2),\"+-\",round((np.std(PEHE_in_list, ddof=1) / np.sqrt(np.size(PEHE_in_list))),2))\n",
        "print(\" Outsample ATE error for HD-net(mean over 10) =\", round(np.mean(ATE_out_list),2),\"+-\",round((np.std(ATE_out_list, ddof=1) / np.sqrt(np.size(ATE_out_list))),2))\n",
        "print(\" Outsample PEHE error for HD-net(mean over 10) =\", round(np.mean(PEHE_out_list),2),\"+-\",round((np.std(PEHE_out_list, ddof=1) / np.sqrt(np.size(PEHE_out_list))),2))\n",
        "print(\" Wall time in seconds for HD-net for 1 simulation(avg over 10) =\", elapsed_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xICGGysZh-s0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}